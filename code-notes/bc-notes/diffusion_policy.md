---
layout: default
title: Lift Task
parent: Behavior Cloning
nav_order: 1
---
# Diffusion Policy Lift Task

[Diffusion Policy Codebase](https://github.com/real-stanford/diffusion_policy/tree/main/diffusion_policy)

Based on <i> lift_image.yaml </i> and it states that dataset comes from the robomimic dataset

---

# Robomimic

[Robomimic Codebase](https://github.com/ARISE-Initiative/robomimic)

After taking a look at RoboMimic, it seems that understanding this paper is essential for understanding how to do BC on a robot. I would recommend anyone getting into BC to read this paper deeply.

### Pre-Code Dive

[Paper link](https://arxiv.org/pdf/2108.03298)

Based on <i> README.md </i> 3/11/2025 robomimic datasets move to HuggingFace.

<b> IMPORTANT </b> from Robomimic, we see that randomizing object in ADDITION to randomizing robot state (by a little) matters in getting robust performance.

<b> IMPORTANT </b> from Robomimic, we see that BC-RNN performs better by a large margin against just BC (concatenate horizon of observations as features). 

![](../../assets/images/code/lift/robomimic_lowdim_obs_table.png)

<i> <b>Figure 1:</b> (MG) is machine generated. (PH) is proficient human. (MH) is multi-human. We can see that lift does really well in general. It is perfect for HUMAN demonstrations. However, it is much worse for machine-generated (MG). Either way, we should expect high performance in our task. </i>


The observation that (PH) and (MH) does better than (MG) is actually stated in the paper in their <b> C1 </b> remark. These (C#) remarks are claims they wish to validate.


> <b> (C1) Data from Non-Markovian Decision Process. </b> Human demonstrations can differ substantially
from machine-generated demonstrations because humans may not act purely based on a single current
observation. External factors (teleoperation device, past actions, history of episode) may all play a
role. Prior work [20] has noted substantial benefits from leveraging models that are history-dependent
and / or with temporal abstraction to learn from human demonstrations. We investigate various design
choices related to such architectures in this study.

This would also mean that for (MG) demos, using BC-RNN might not work.


> <b> (C3) Dependence on Dataset Size. </b> Offline policy learning is sensitive to the state and action space
coverage in the dataset, and by extension, the size of the dataset itself. In our study, we investigate
how dataset sizes affect policy performance. This analysis is useful to understand the value of adding
more data â€“ an important consideration since collecting human demonstrations can be costly.

> <b> (C4) Mismatch between Training and Evaluation Objectives. </b> Unlike traditional supervised learning, where model selection can be achieved by using the model with the lowest validation loss [21],
offline policy learning often suffers from the fact that the training objective is only a surrogate for the
true objective of interest (e.g. task success rate), and policy performance can change significantly
from epoch to epoch. This makes it difficult to select the best trained model [19, 28, 29]. In our
study, we evaluate each policy checkpoint online in the environment in simulation, and report the best
policy success rate per training run. We use these ground-truth values to understand the effectiveness
of different selection criteria, and confirm that offline policy selection is an important problem,
especially in real-world scenarios where large-scale empirical evaluation is difficult.

<b> IMPORTANT BIG NOTE </b> (MG) is generated by running a SOTA RL Policy (from 2021) on these tasks. These literally take in one observation horizon and output an action. We should not trust the (MG) results on scripted demos.


![](../../assets/images/code/lift/robomimic-bc-rnn-study.png)

These figures don't need much explaning. It just shows how different hyperparameter settings can change the policy's performance (mostly for the worse here).

Now we move into the appendix section of the paper because that is where the juicy pre-code details are.

#### Appendix Info

Policy outputs actions at a rate of 20Hz. Action space is in delta-pose form. Delta-rotation is axis-angle. 

> <b> Lift. </b> Object observations (10-dim) consist of the absolute cube position and cube quaternion (7-dim),
and the cube position relative to the robot end effector (3-dim). The cube pose is randomized at the
start of each episode with a random z-rotation in a small square region at the center of the table.

<b> Simulation Note: </b> All tasks were designed using MuJoCo and the robosuite framework. 

All of the simulation code is actually from robosuite codebase which wraps MuJoCo.

The action horizon was 1 by the way.

---

## Robomimic Code Dive